{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c18aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE  # NEW: Import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # NEW: Use imblearn's pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. Load data and Feature Engineering ---\n",
    "print(\"--- 1. Loading and Feature Engineering Data ---\")\n",
    "df = pd.read_pickle('../data/feature_engineered_transactions.pkl')\n",
    "print(f\"Data loaded: Rows={len(df)}, Columns={len(df.columns)}\")\n",
    "\n",
    "# --- NEW: Feature Engineering - Velocity by IP (Transactions in last 24 hours) ---\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']) # Ensure timestamp is datetime\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate transaction count per IP in the preceding 24 hours\n",
    "df['ip_txn_count_prev_24h'] = df.groupby('ip_address').rolling(\n",
    "    '24h', on='timestamp', closed='left')['transaction_id'].count().reset_index(level=0, drop=True)\n",
    "df['ip_txn_count_prev_24h'] = df['ip_txn_count_prev_24h'].fillna(0).astype('int32')\n",
    "print(\"New feature 'ip_txn_count_prev_24h' created.\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df.drop(['is_fraud', 'timestamp'], axis=1) # Drop 'timestamp' now\n",
    "y = df['is_fraud']\n",
    "identifiers = ['customer_id', 'device_id', 'transaction_id', 'ip_address'] # Keep 'ip_address' for CatBoost later, but remove it as an identifier here\n",
    "X = X.drop(columns=[c for c in identifiers if c in X.columns])\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64', 'int32']).columns.tolist()\n",
    "print(f\"Numerical: {len(numerical_features)}, Categorical: {len(categorical_features)}\")\n",
    "\n",
    "# --- 2. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\"Train={len(X_train)}, Test={len(X_test)}, Fraud rate={y_train.mean():.4f}\")\n",
    "print(f\"Original Training Class Distribution: {Counter(y_train)}\")\n",
    "\n",
    "# --- 3. Preprocessing for XGBoost & LightGBM (using imblearn pipeline) ---\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# --- 4. Function to evaluate models ---\n",
    "def evaluate_model(model, X_test, y_test, model_name, preprocessor=None):\n",
    "    if preprocessor:\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:,1]\n",
    "    else:\n",
    "        # For CatBoost without preprocessor\n",
    "        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    f1_scores = 2*(precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1])\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "    \n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    y_pred_opt = (y_pred_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}, AUC-PR: {auc_pr:.4f}, Best threshold={best_threshold:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_opt, target_names=['Non-Fraud','Fraud']))\n",
    "    \n",
    "    report_dict = classification_report(y_test, y_pred_opt, target_names=['Non-Fraud','Fraud'], output_dict=True)\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model_object': model,\n",
    "        'auc_roc': auc_roc,\n",
    "        'auc_pr': auc_pr,\n",
    "        'f1_fraud': report_dict['Fraud']['f1-score'],\n",
    "        'precision_fraud': report_dict['Fraud']['precision'],\n",
    "        'recall_fraud': report_dict['Fraud']['recall']\n",
    "    }\n",
    "\n",
    "# --- 5. XGBoost with SMOTE Pipeline ---\n",
    "print(\"\\n--- 5. XGBoost with SMOTE Pipeline ---\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    use_label_encoder=False,\n",
    "    # REMOVED: We no longer use scale_pos_weight because SMOTE balances the data.\n",
    ")\n",
    "\n",
    "# NEW: Create a pipeline that applies preprocessing, then SMOTE, then the model\n",
    "xgb_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42, n_jobs=-1)),\n",
    "    ('classifier', xgb_model)\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# We pass the preprocessor to the evaluation function so it can process X_test\n",
    "results = [evaluate_model(xgb_pipeline, X_test, y_test, \"XGBoost (SMOTE)\", preprocessor=None)] \n",
    "# Note: ImbPipeline handles the preprocessor step internally, so pass None here\n",
    "\n",
    "# --- 6. LightGBM with SMOTE Pipeline ---\n",
    "print(\"\\n--- 6. LightGBM with SMOTE Pipeline ---\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    # REMOVED: We no longer use is_unbalance=True because SMOTE balances the data.\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42, n_jobs=-1)),\n",
    "    ('classifier', lgb_model)\n",
    "])\n",
    "\n",
    "lgb_pipeline.fit(X_train, y_train)\n",
    "results.append(evaluate_model(lgb_pipeline, X_test, y_test, \"LightGBM (SMOTE)\", preprocessor=None))\n",
    "\n",
    "# --- 7. CatBoost (Keeping the original approach for comparison) ---\n",
    "print(\"\\n--- 7. CatBoost (Original Approach with new feature) ---\")\n",
    "# CatBoost handles categorical features natively, so we don't need the preprocessor/SMOTE pipeline\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    eval_metric='F1',\n",
    "    verbose=0,\n",
    "    early_stopping_rounds=50,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    allow_writing_files=False,\n",
    "    depth=6, # Using the best depth from your previous tuning\n",
    "    learning_rate=0.03, # Using the best LR from your previous tuning\n",
    "    random_seed=42)\n",
    "\n",
    "cat_model.fit(X_train, y_train, cat_features=categorical_features)\n",
    "results.append(evaluate_model(cat_model, X_test, y_test, \"CatBoost (Weighted)\"))\n",
    "\n",
    "\n",
    "# --- 8. Final Model Selection and Saving ---\n",
    "print(\"\\n--- 8. Final Model Performance Summary ---\")\n",
    "results_df = pd.DataFrame(results).sort_values(by='f1_fraud', ascending=False)\n",
    "best_row = results_df.iloc[0]\n",
    "final_model = best_row['model_object']\n",
    "final_model_name = best_row['model_name']\n",
    "\n",
    "print(results_df[['model_name', 'auc_roc', 'auc_pr', 'f1_fraud', 'precision_fraud', 'recall_fraud']].to_string(index=False))\n",
    "\n",
    "# --- 9. Save the Best Model and Preprocessor ---\n",
    "print(f\"\\n--- 9. Saving Selected Model: {final_model_name} ---\")\n",
    "\n",
    "if final_model_name.startswith('CatBoost'):\n",
    "    joblib.dump(final_model, f'../models/{final_model_name}_fraud_model.joblib')\n",
    "    print(f\"Final model ({final_model_name}) saved to '../models/{final_model_name}_fraud_model.joblib'.\")\n",
    "    print(\"No preprocessor saved, as CatBoost handles features natively.\")\n",
    "else:\n",
    "    # Save the entire imblearn pipeline object for XGBoost/LightGBM\n",
    "    joblib.dump(final_model, f'../models/{final_model_name}_fraud_model_pipeline.joblib')\n",
    "    print(f\"Final model pipeline ({final_model_name}) saved to '../models/{final_model_name}_fraud_model_pipeline.joblib'.\")\n",
    "\n",
    "print(\"--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
