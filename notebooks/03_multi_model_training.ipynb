{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14baabf7",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac517ec",
   "metadata": {},
   "source": [
    "# --- 1. Load data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf18cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Feature Engineered Data ---\n",
      "Data loaded: Rows=10200, Columns=36\n",
      "Numerical: 23, Categorical: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Feature Engineered Data ---\")\n",
    "df = pd.read_pickle('../data/feature_engineered_transactions.pkl')\n",
    "print(f\"Data loaded: Rows={len(df)}, Columns={len(df.columns)}\")\n",
    "\n",
    "X = df.drop('is_fraud', axis=1)\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Drop identifiers\n",
    "identifiers = ['customer_id', 'device_id', 'transaction_id']\n",
    "X = X.drop(columns=[c for c in identifiers if c in X.columns])\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64', 'int32']).columns.tolist()\n",
    "print(f\"Numerical: {len(numerical_features)}, Categorical: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c1dc4",
   "metadata": {},
   "source": [
    "# --- 2. Train/test split ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e33b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train=8160, Test=2040, Fraud rate=0.0194\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train={len(X_train)}, Test={len(X_test)}, Fraud rate={y_train.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17060a",
   "metadata": {},
   "source": [
    "# --- 3. Preprocessing for XGBoost & LightGBM ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eda8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # FIX\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07301e64",
   "metadata": {},
   "source": [
    "# --- 4. Function to evaluate models ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6c5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    f1_scores = 2*(precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1])\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    y_pred_opt = (y_pred_proba >= best_threshold).astype(int)\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}, AUC-PR: {auc_pr:.4f}, Best threshold={best_threshold:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_opt, target_names=['Non-Fraud','Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005041c6",
   "metadata": {},
   "source": [
    "# --- 5. XGBoost ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f32dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\kinghenrymorgan_analytics_core\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:14:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost ---\n",
      "AUC-ROC: 0.7148, AUC-PR: 0.0848, Best threshold=0.3090\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Fraud       0.98      0.97      0.98      2001\n",
      "       Fraud       0.12      0.23      0.16        39\n",
      "\n",
      "    accuracy                           0.95      2040\n",
      "   macro avg       0.55      0.60      0.57      2040\n",
      "weighted avg       0.97      0.95      0.96      2040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=(len(y_train)-y_train.sum())/y_train.sum()\n",
    ")\n",
    "xgb_model.fit(X_train_processed, y_train)\n",
    "evaluate_model(xgb_model, X_test_processed, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e7d79",
   "metadata": {},
   "source": [
    "# --- 6. LightGBM ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 158, number of negative: 8002\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1726\n",
      "[LightGBM] [Info] Number of data points in the train set: 8160, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019363 -> initscore=-3.924852\n",
      "[LightGBM] [Info] Start training from score -3.924852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "--- LightGBM ---\n",
      "AUC-ROC: 0.6814, AUC-PR: 0.0723, Best threshold=0.4379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Fraud       0.98      0.98      0.98      2001\n",
      "       Fraud       0.12      0.13      0.12        39\n",
      "\n",
      "    accuracy                           0.97      2040\n",
      "   macro avg       0.55      0.55      0.55      2040\n",
      "weighted avg       0.97      0.97      0.97      2040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\kinghenrymorgan_analytics_core\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    is_unbalance=True,  \n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train_processed, y_train)\n",
    "evaluate_model(lgb_model, X_test_processed, y_test, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c856a",
   "metadata": {},
   "source": [
    "# --- 7. CatBoost ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c060f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. CatBoost Hyperparameter Tuning (Randomized Search) ---\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "--- CatBoost (Tuned) ---\n",
      "AUC-ROC: 0.7150, AUC-PR: 0.0700, Best threshold=0.6705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Fraud       0.98      0.99      0.98      2001\n",
      "       Fraud       0.15      0.13      0.14        39\n",
      "\n",
      "    accuracy                           0.97      2040\n",
      "   macro avg       0.57      0.56      0.56      2040\n",
      "weighted avg       0.97      0.97      0.97      2040\n",
      "\n",
      "\n",
      "--- CatBoost (Tuned) ---\n",
      "AUC-ROC: 0.7150, AUC-PR: 0.0700, Best threshold=0.6705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Fraud       0.98      0.99      0.98      2001\n",
      "       Fraud       0.15      0.13      0.14        39\n",
      "\n",
      "    accuracy                           0.97      2040\n",
      "   macro avg       0.57      0.56      0.56      2040\n",
      "weighted avg       0.97      0.97      0.97      2040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1320\\3223829741.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*(precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1320\\3305676133.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*(precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1])\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Hyperparameter Tuning for CatBoost (Best Performer) ---\n",
    "print(\"\\n--- 7. CatBoost Hyperparameter Tuning (Randomized Search) ---\")\n",
    "\n",
    "# Initialize results list to collect model performance metrics\n",
    "results = []\n",
    "\n",
    "# Calculate class weight for imbalanced data\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "\n",
    "# Define the parameter grid for Randomized Search\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'depth': [5, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5, 10],\n",
    "    'random_seed': [42]\n",
    "}\n",
    "\n",
    "base_cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    eval_metric='F1',\n",
    "    verbose=0,\n",
    "    early_stopping_rounds=50,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_cat_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring=f1_scorer,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the search on the raw training data (CatBoost handles categories)\n",
    "random_search.fit(X_train, y_train, cat_features=categorical_features)\n",
    "\n",
    "best_cat_model = random_search.best_estimator_\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_proba = best_cat_model.predict_proba(X_test)[:,1]\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "auc_pr = auc(recall, precision)\n",
    "f1_scores = 2*(precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1])\n",
    "f1_scores = np.nan_to_num(f1_scores)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "y_pred_opt = (y_pred_proba >= best_threshold).astype(int)\n",
    "print(f\"\\n--- CatBoost (Tuned) ---\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}, AUC-PR: {auc_pr:.4f}, Best threshold={best_threshold:.4f}\")\n",
    "print(classification_report(y_test, y_pred_opt, target_names=['Non-Fraud','Fraud']))\n",
    "\n",
    "# Extract metrics for results tracking\n",
    "report_dict = classification_report(y_test, y_pred_opt, target_names=['Non-Fraud','Fraud'], output_dict=True)\n",
    "results.append({\n",
    "    'model_name': 'CatBoost (Tuned)',\n",
    "    'model_object': best_cat_model,\n",
    "    'auc_roc': auc_roc,\n",
    "    'auc_pr': auc_pr,\n",
    "    'f1_fraud': report_dict['Fraud']['f1-score'],\n",
    "    'precision_fraud': report_dict['Fraud']['precision'],\n",
    "    'recall_fraud': report_dict['Fraud']['recall']\n",
    "})\n",
    "# Evaluate the best model on the test set\n",
    "results.append(evaluate_model(best_cat_model, X_test, y_test, \"CatBoost (Tuned)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade80eeb",
   "metadata": {},
   "source": [
    "# --- 8. Save models ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ae2f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Final Model Performance Summary ---\n",
      "      model_name  auc_roc   auc_pr  f1_fraud  precision_fraud  recall_fraud\n",
      "CatBoost (Tuned)  0.71495 0.070049  0.138889         0.151515      0.128205\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 8. Final Model Performance Summary ---\")\n",
    "results = [r for r in results if r is not None] \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"ERROR: No models successfully trained or evaluated. Cannot proceed with final selection.\")\n",
    "else:\n",
    "    # Selecting the model with the highest F1-Score for the fraud class\n",
    "    best_row = results_df.sort_values(by='f1_fraud', ascending=False).iloc[0]\n",
    "    final_model = best_row['model_object']\n",
    "    final_model_name = best_row['model_name']\n",
    "\n",
    "    print(results_df[['model_name', 'auc_roc', 'auc_pr', 'f1_fraud', 'precision_fraud', 'recall_fraud']].sort_values(by='f1_fraud', ascending=False).to_string(index=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67268c",
   "metadata": {},
   "source": [
    "# --- 9. Feature Importance Analysis for the Best Model (XGBoost) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e082c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 9. Feature Importance Analysis for Selected Model: CatBoost (Tuned) ---\n",
      "Final model (CatBoost (Tuned)) saved to '../models/CatBoost (Tuned)_fraud_model.joblib'.\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                  feature  importance\n",
      "            ip_risk_score   10.877432\n",
      "         txn_day_of_month    8.906903\n",
      "      risk_score_internal    7.733498\n",
      "      mean_amount_prev_3d    7.666782\n",
      "       device_trust_score    6.879754\n",
      "                      fee    6.639405\n",
      "                 txn_hour    5.778870\n",
      "exchange_rate_src_to_dest    4.941788\n",
      "          txn_day_of_week    4.606931\n",
      "        txn_count_prev_3d    4.501585\n",
      "         account_age_days    4.286968\n",
      "                  channel    4.055739\n",
      "               amount_src    3.784763\n",
      "               amount_usd    3.645717\n",
      "                 kyc_tier    2.310178\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- 9. Feature Importance Analysis for Selected Model: {final_model_name} ---\")\n",
    "\n",
    "if final_model_name == 'XGBoost':\n",
    "        # This model uses the preprocessor, so we extract feature names from it\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        importance = final_model.feature_importances_\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        # Save the preprocessor as the best model needs it\n",
    "        joblib.dump(preprocessor, '../models/preprocessor.joblib')\n",
    "        print(\"Preprocessor saved to '../models/preprocessor.joblib'.\")\n",
    "\n",
    "        # Save the best model\n",
    "        joblib.dump(final_model, f'../models/{final_model_name}_fraud_model.joblib')\n",
    "        print(f\"Final model ({final_model_name}) saved to '../models/{final_model_name}_fraud_model.joblib'.\")\n",
    "\n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "elif final_model_name.startswith('CatBoost'):\n",
    "        importance = final_model.get_feature_importance()\n",
    "        feature_names = X_train.columns\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        joblib.dump(final_model, f'../models/{final_model_name}_fraud_model.joblib')\n",
    "        print(f\"Final model ({final_model_name}) saved to '../models/{final_model_name}_fraud_model.joblib'.\")\n",
    "\n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        print(importance_df.head(15).to_string(index=False))\n",
    "        \n",
    "else:\n",
    "        print(\"Selected model is LightGBM or another type. Skipping detailed feature importance extraction.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
